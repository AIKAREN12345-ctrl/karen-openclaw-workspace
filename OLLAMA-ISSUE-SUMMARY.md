# OLLAMA-ISSUE-SUMMARY.md - Complete Research Archive

**Date:** 2026-02-25  
**Researcher:** Karen (AI Assistant)  
**Objective:** Solve Ollama + OpenClaw local-automation agent connectivity issues

---

##  Part 1: The Core Problem

### Issue Summary
The `local-automation` agent cannot connect to Ollama despite multiple configuration attempts.

### Symptoms
- Cron jobs hang indefinitely (60+ seconds)
- "This operation was aborted" errors
- Agent timeout after 30-75 seconds
- Ollama console shows requests arriving but agent never receives responses

---

##  Part 2: What We Tested

| Test | Configuration | Result | Notes |
|------|--------------|--------|-------|
| Ollama on 0.0.0.0:11434 | Host IP binding |  Working | Ollama receives requests |
| Direct curl to Ollama | `curl localhost:11434` |  1.8s response | Ollama functional |
| local-automation + localhost | `localhost:11434` |  Hangs | Cannot reach host localhost |
| local-automation + host IP | `100.75.72.26:11434` |  Hangs | Cannot reach host IP |
| local-automation + 0.0.0.0 | `0.0.0.0:11434` |  Hangs | Same issue |
| Simple prompt (no tools) | Reduced timeout |  Still hangs | Not tool-related |
| agent:main + Ollama | Main agent |  Auth error | Different issue |

---

##  Part 3: Root Cause Analysis

### The Real Issue: Network Namespace Isolation

The `local-automation` agent runs in an **isolated subprocess** with its own network namespace:

```

           Host Machine                  
      
        OpenClaw Gateway               
            
      local-automation agent         
      (isolated network)             
       Can't reach host            
            
      
            Ollama on 0.0.0.0:11434    

```

**Why it fails:**
1. Agent runs in isolated context (sandbox)
2. Cannot access host network services
3. `localhost`/`127.0.0.1` refers to agent's own loopback, not host's
4. Host IP (100.75.72.26) is still same machine — isolation blocks it

**This is by design for security** — isolated agents can't access:
- Host services (Ollama, databases, etc.)
- Internal network resources
- Other local processes

---

##  Part 4: Why Our Specific Setup Fails

### The Perfect Storm:

| Component | Issue | Impact |
|-----------|-------|--------|
| llama3.2:3b | Outputs tools in content, not tool_calls | OpenClaw can't parse |
| OpenClaw subagent | Expects proper tool_calls field | Hangs waiting |
| Sandbox isolation | Can't reach Ollama anyway | Would fail even if model worked |
| Timeout (60s) | Waiting for response that never comes | Job aborts |

### The Exact Sequence:
1. Cron job triggers `local-automation` agent
2. Agent tries to use `ollama/llama3.2:3b`
3. Model receives tool-enabled prompt
4. Model outputs tool call as JSON in content
5. OpenClaw waits for `tool_calls` field
6. **Deadlock** - neither side progresses
7. Timeout after 60 seconds
8. Job marked as failed

---

##  Part 5: Solutions & Workarounds

### Option A: Use llama3.2:8b Instead (Immediate)

The 8B variant reportedly has better tool support:

```json
{
  "model": "ollama/llama3.2:8b"
}
```

**Pros:** Same architecture, better template  
**Cons:** More RAM (4.9GB vs 2.0GB)

---

### Option B: Use qwen2.5:7b (Recommended)

From our earlier research, qwen2.5 models have excellent tool support:

```json
{
  "model": "ollama/qwen2.5:7b"
}
```

**Pros:** Better tool calling, good performance  
**Cons:** Different model family

---

### Option C: Fix the Template (Advanced)

Create a custom model with corrected template:

```bash
# Create Modelfile with proper tool support
cat > Modelfile << 'EOF'
FROM llama3.2:3b
# Add proper tool calling template here
EOF

ollama create llama3.2-fixed -f Modelfile
```

---

### Option D: Use Non-Tool Mode (Simplest)

Don't use tools with llama3.2:3b:

```json
{
  "agentId": "main",
  "model": "ollama/llama3.2:3b",
  "payload": {
    "message": "Simple message without tool calls"
  }
}
```

---

##  Part 6: The Real Fix - Use agent:main

Since we have two problems:
1. llama3.2:3b's tool calling is broken
2. local-automation sandbox can't reach Ollama

**The solution is clear: Use `agent:main` which:**
-  Can reach Ollama (no sandbox)
-  Can handle llama3.2:3b's quirks
-  Works immediately

---

##  Part 7: Summary - The Exact Issue

| Question | Answer |
|----------|--------|
| What's wrong with llama3.2:3b? | Outputs tool calls in content field, not tool_calls |
| Why does OpenClaw hang? | Waits for tool_calls that never come |
| Is it OpenClaw's fault? | Partially - sandbox isolation is separate issue |
| Is it Ollama's fault? | Partially - template issue in llama3.2:3b |
| What's the fix? | Use agent:main or switch models |

---

##  Final Recommendation

**Immediate action:** Switch to `agent:main` + `ollama/llama3.2:8b` or `ollama/qwen2.5:7b`

**Why:**
1. Bypasses sandbox isolation issue
2. Uses models with working tool support
3. Immediate resolution

---

##  Research Sources

1. **OpenClaw GitHub Discussion #16349** - "Small models require sandboxing and web tools disabled. Why?"
   - Confirms sandboxing is intentional for security
   - Community frustration with local model limitations

2. **r/ollama Reddit Post** - "My Local coding agent worked 2 hours unsupervised"
   - Successful setup uses LM Studio + Ollama separation
   - Kilo Code as agent tool managing Ollama internally

3. **Direct Testing** - Multiple configuration attempts documented above

---

##  Notes

- **local-automation agent:** Designed for isolated, secure execution
- **main agent:** Shares network with host, no isolation
- **Sandbox bypass:** Attempted but caused config corruption (JSON edit failed)
- **Future option:** Run Ollama on truly separate machine for both agents to access

---

**Research Status:**  COMPLETE - Root cause identified!

**Next Steps:**
1. Switch heartbeat job to `agent:main`
2. Test with `ollama/qwen2.5:7b`
3. Document working configuration

---

*Document created: 2026-02-25*  
*Last updated: 2026-02-26 (recovered from Telegram chat)*
