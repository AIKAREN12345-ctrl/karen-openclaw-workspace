# RESEARCH-2026-02-26.md - OpenClaw Skills System

**Date:** 2026-02-26  
**Researcher:** Karen  
**Topics:** OpenClaw skills, GitHub integration, system upgrades, local automation

---

## 1. OpenClaw Skills System

### What Are Skills?
OpenClaw uses **AgentSkills**-compatible skill folders to teach the agent how to use tools. Each skill is a directory containing a `SKILL.md` with YAML frontmatter and instructions.

### Skill Locations (Precedence Order)
1. **Workspace skills** (highest): `<workspace>/skills`
2. **Managed/local skills**: `~/.openclaw/skills`
3. **Bundled skills** (lowest): shipped with OpenClaw

### Skill Format (SKILL.md)
```yaml
---
name: skill-name
description: What this skill does
metadata: {"openclaw": {"requires": {"bins": ["tool-name"]}}}
---

Instructions for using the skill...
```

### Key Features
- **ClawHub**: Public skills registry at https://clawhub.com
- **Gating**: Skills can require specific binaries, env vars, or config
- **Auto-refresh**: Skills watcher reloads on file changes
- **Per-agent vs shared**: Workspace skills are per-agent, ~/.openclaw/skills are shared

### Commands
```bash
clawhub install <skill-slug>    # Install a skill
clawhub update --all            # Update all skills
clawhub sync --all              # Sync skills
```

---

## 2. GitHub Integration

### Current Capabilities
From the research, OpenClaw has:
- GitHub releases integration
- GitHub source code access
- Community plugins

### Potential Uses
- Pull code from GitHub repositories
- Monitor GitHub releases for updates
- Use GitHub Actions for automation
- Store skills/configs in GitHub repos

---

## 3. System Upgrade Methods

### OpenClaw Update Process
```bash
npm i -g openclaw@latest    # Update OpenClaw
openclaw gateway install --force  # Sync service token
openclaw gateway restart    # Restart gateway
```

### Configuration Management
- Main config: `~/.openclaw/openclaw.json`
- Node config: `~/.openclaw/node.json`
- Skills: `~/.openclaw/skills/`
- Cron jobs: `~/.openclaw/cron/jobs.json`

### Backup Strategy
1. Git commit workspace changes
2. Document procedures in START-HERE.md
3. Keep config backups (.bak files)

---

## 4. Local Automation System Design

### Current Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Host Machine (Windows)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚      OpenClaw Gateway           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚  main agent             â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  (shared network)       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  âœ… Can reach Ollama    â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚  local-automation agent â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  (isolated network)     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  âŒ Can't reach Ollama  â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           âœ… Ollama on 0.0.0.0:11434    â”‚
â”‚           âœ… VNC Server                 â”‚
â”‚           âœ… Node.js / Python           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Working Local Automation Pattern
Based on research from r/ollama and GitHub discussions:

**Option A: Use `agent:main` for Ollama (Recommended)**
- Main agent shares network with host
- No sandbox restrictions
- Direct Ollama access

**Option B: Separate Node Architecture**
- Run Ollama on different machine/port
- Both agents can reach external endpoint
- True network separation

**Option C: External Tools (Kilo Code, LM Studio)**
- Purpose-built for local LLM coding
- Manage Ollama access internally
- Better tool integration

### Recommended Models for Local Automation
| Model | Tool Support | RAM | Notes |
|-------|--------------|-----|-------|
| qwen2.5:7b | âœ… Excellent | 4.7GB | Recommended |
| llama3.2:8b | âœ… Good | 4.9GB | Better than 3b |
| llama3.2:3b | âŒ Broken | 2.0GB | Wrong output format |

### Key Insight from Research
The `local-automation` agent's network isolation is **by design for security**. The community workaround is using non-isolated agents (`agent:main`) or external tools that handle LLM access internally.

---

## 5. Reddit Community Research (r/ollama)

### Post: "I ran ClawBot with Ollama locally on my Mac" (191 upvotes)
**Source:** https://www.reddit.com/r/ollama/comments/1rc3srb/

**TL;DR from OP:**
- âœ… ClawBot + Ollama works on Mac
- âš ï¸ 8B models limit agentic tasks - basic Q&A fine, complex tasks hit ceiling
- â±ï¸ Speed is slow - "go make a coffee while you wait"
- ðŸ’¡ Worth it for learning and experimenting locally for free

**What Worked:**
- Clean setup: VS Code, JSON config, localhost dashboard
- Ollama model switching is straightforward
- Good for understanding local AI agent architecture

**What Didn't:**
- Speed is rough on anything under 32GB RAM
- 8B models hit ceiling fast on multi-step reasoning
- Keep context window low (but see conflicting advice below)

### Community Insights from Comments

**1. Security Concerns (47 upvotes)**
> "Please don't do this on your personal system... The number of security vulnerabilities associated with this software is astounding."
> 
> Response: "it's absolutely insane! it's like 700+ and so many CRITICAL ones"

**Implication:** Our security hardening (rate limiting, Telegram allowlist) is essential. Running in isolated environment recommended.

**2. Alternative Backends (2 upvotes)**
> "I would run llama.cpp instead to get a few more T/s, or possibly vLLM."
> 
> "Reducing context speeds things up, but it makes the models unfit for longer tasks."

**Implication:** Ollama may not be the fastest option. llama.cpp or vLLM could provide better performance.

**3. Context Window Debate (6 upvotes)**
> "Why do you recommend keeping the context window low? 32GB is plenty for 8B model... My findings with Openclaw and Ollama is exactly the opposite. Increase the context window size and the 'bot' will get much better."

**Response:**
> "I'm running ministral3-14b on a mac mini 16gb as the daily task master... I have reached 35k context without it even getting warm. I'm also running bge-m3 embedded for memory."

**Implication:** OP's advice to keep context low may be wrong. Community suggests larger context improves performance.

**4. Hardware Setups Mentioned:**
- Mac Mini + 16GB RAM + ministral3-14b
- 5090 with 32GB VRAM + 5070 Ti with 16GB (trying to combine for 48GB)
- 6000 Blackwell for benchmarking

### Related Posts Found
1. **"Is my Mac Mini + Ollama LLM setup good enough for Openclaw?"** (8 upvotes, 30 comments)
2. **"Anyone running OpenClaw fully local with Ollama?"** (7 upvotes, 10 comments) - r/openclaw
3. **"Running Openclaw + Ollama locally with old hardware"** (18 upvotes, 15 comments)
4. **"Ollama and Openclaw on separate dedicated, isolated, firewalled machines"** (6 upvotes, 11 comments)

---

## 6. Action Items

### Immediate
- [ ] Switch local automation jobs to `agent:main`
- [ ] Test with `ollama/qwen2.5:7b`
- [ ] Document working configuration
- [ ] Research llama.cpp vs Ollama performance comparison

### Short-term
- [ ] Explore Kilo Code integration
- [ ] Research LM Studio as Ollama alternative
- [ ] Set up Brave API key for web_search
- [ ] Test larger context windows (contrary to OP's advice)

### Long-term
- [ ] Create custom skills for common tasks
- [ ] Publish useful skills to ClawHub
- [ ] Automate skill updates via cron
- [ ] Consider vLLM for production use

---

## 7. Resources

- **ClawHub**: https://clawhub.com
- **OpenClaw Docs**: https://docs.openclaw.ai/skills
- **AgentSkills Spec**: https://agentskills.io
- **GitHub**: https://github.com/openclaw/openclaw
- **Reddit r/ollama**: https://reddit.com/r/ollama
- **Reddit r/openclaw**: https://reddit.com/r/openclaw

---

## 8. Key Decisions Pending

| Decision | Options | Considerations |
|----------|---------|----------------|
| LLM Backend | Ollama / llama.cpp / vLLM | Speed vs compatibility |
| Model Size | 7B / 8B / 14B | RAM constraints, tool support |
| Context Window | Low / High | Speed vs capability |
| Agent Type | main / local-automation | Security vs functionality |
| Hardware | Current / Upgrade | Cost vs performance |

---

**Research Status:** âœ… Ongoing - Reddit phase complete  
**Next Phase:** GitHub integration research, alternative backend evaluation

---

*Document created: 2026-02-26*  
*Last updated: 2026-02-26 (Reddit research added)*
