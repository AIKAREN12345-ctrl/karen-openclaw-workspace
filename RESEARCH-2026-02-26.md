# RESEARCH-2026-02-26.md - OpenClaw Skills System

**Date:** 2026-02-26  
**Researcher:** Karen  
**Topics:** OpenClaw skills, GitHub integration, system upgrades, local automation

---

## 1. OpenClaw Skills System

### What Are Skills?
OpenClaw uses **AgentSkills**-compatible skill folders to teach the agent how to use tools. Each skill is a directory containing a `SKILL.md` with YAML frontmatter and instructions.

### Skill Locations (Precedence Order)
1. **Workspace skills** (highest): `<workspace>/skills`
2. **Managed/local skills**: `~/.openclaw/skills`
3. **Bundled skills** (lowest): shipped with OpenClaw

### Skill Format (SKILL.md)
```yaml
---
name: skill-name
description: What this skill does
metadata: {"openclaw": {"requires": {"bins": ["tool-name"]}}}
---

Instructions for using the skill...
```

### Key Features
- **ClawHub**: Public skills registry at https://clawhub.com
- **Gating**: Skills can require specific binaries, env vars, or config
- **Auto-refresh**: Skills watcher reloads on file changes
- **Per-agent vs shared**: Workspace skills are per-agent, ~/.openclaw/skills are shared

### Commands
```bash
clawhub install <skill-slug>    # Install a skill
clawhub update --all            # Update all skills
clawhub sync --all              # Sync skills
```

---

## 2. GitHub Integration

### Current Capabilities
From the research, OpenClaw has:
- GitHub releases integration
- GitHub source code access
- Community plugins

### Potential Uses
- Pull code from GitHub repositories
- Monitor GitHub releases for updates
- Use GitHub Actions for automation
- Store skills/configs in GitHub repos

---

## 3. System Upgrade Methods

### OpenClaw Update Process
```bash
npm i -g openclaw@latest    # Update OpenClaw
openclaw gateway install --force  # Sync service token
openclaw gateway restart    # Restart gateway
```

### Configuration Management
- Main config: `~/.openclaw/openclaw.json`
- Node config: `~/.openclaw/node.json`
- Skills: `~/.openclaw/skills/`
- Cron jobs: `~/.openclaw/cron/jobs.json`

### Backup Strategy
1. Git commit workspace changes
2. Document procedures in START-HERE.md
3. Keep config backups (.bak files)

---

## 4. Local Automation System Design

### Current Architecture
```

           Host Machine (Windows)        
      
        OpenClaw Gateway               
            
      main agent                     
      (shared network)               
       Can reach Ollama            
            
            
      local-automation agent         
      (isolated network)             
       Can't reach Ollama          
            
      
            Ollama on 0.0.0.0:11434    
            VNC Server                 
            Node.js / Python           

```

### Working Local Automation Pattern
Based on research from r/ollama and GitHub discussions:

**Option A: Use `agent:main` for Ollama (Recommended)**
- Main agent shares network with host
- No sandbox restrictions
- Direct Ollama access

**Option B: Separate Node Architecture**
- Run Ollama on different machine/port
- Both agents can reach external endpoint
- True network separation

**Option C: External Tools (Kilo Code, LM Studio)**
- Purpose-built for local LLM coding
- Manage Ollama access internally
- Better tool integration

### Recommended Models for Local Automation
| Model | Tool Support | RAM | Notes |
|-------|--------------|-----|-------|
| qwen2.5:7b |  Excellent | 4.7GB | Recommended |
| llama3.2:8b |  Good | 4.9GB | Better than 3b |
| llama3.2:3b |  Broken | 2.0GB | Wrong output format |

### Key Insight from Research
The `local-automation` agent's network isolation is **by design for security**. The community workaround is using non-isolated agents (`agent:main`) or external tools that handle LLM access internally.

---

## 5. Reddit Community Research (r/ollama)

### Post: "I ran ClawBot with Ollama locally on my Mac" (191 upvotes)
**Source:** https://www.reddit.com/r/ollama/comments/1rc3srb/

**TL;DR from OP:**
-  ClawBot + Ollama works on Mac
-  8B models limit agentic tasks - basic Q&A fine, complex tasks hit ceiling
- â± Speed is slow - "go make a coffee while you wait"
-  Worth it for learning and experimenting locally for free

**What Worked:**
- Clean setup: VS Code, JSON config, localhost dashboard
- Ollama model switching is straightforward
- Good for understanding local AI agent architecture

**What Didn't:**
- Speed is rough on anything under 32GB RAM
- 8B models hit ceiling fast on multi-step reasoning
- Keep context window low (but see conflicting advice below)

### Community Insights from Comments

**1. Security Concerns (47 upvotes)**
> "Please don't do this on your personal system... The number of security vulnerabilities associated with this software is astounding."
> 
> Response: "it's absolutely insane! it's like 700+ and so many CRITICAL ones"

**Implication:** Our security hardening (rate limiting, Telegram allowlist) is essential. Running in isolated environment recommended.

**2. Alternative Backends (2 upvotes)**
> "I would run llama.cpp instead to get a few more T/s, or possibly vLLM."
> 
> "Reducing context speeds things up, but it makes the models unfit for longer tasks."

**Implication:** Ollama may not be the fastest option. llama.cpp or vLLM could provide better performance.

**3. Context Window Debate (6 upvotes)**
> "Why do you recommend keeping the context window low? 32GB is plenty for 8B model... My findings with Openclaw and Ollama is exactly the opposite. Increase the context window size and the 'bot' will get much better."

**Response:**
> "I'm running ministral3-14b on a mac mini 16gb as the daily task master... I have reached 35k context without it even getting warm. I'm also running bge-m3 embedded for memory."

**Implication:** OP's advice to keep context low may be wrong. Community suggests larger context improves performance.

**4. Hardware Setups Mentioned:**
- Mac Mini + 16GB RAM + ministral3-14b
- 5090 with 32GB VRAM + 5070 Ti with 16GB (trying to combine for 48GB)
- 6000 Blackwell for benchmarking

### Related Posts Found
1. **"Is my Mac Mini + Ollama LLM setup good enough for Openclaw?"** (8 upvotes, 30 comments)
2. **"Anyone running OpenClaw fully local with Ollama?"** (7 upvotes, 10 comments) - r/openclaw
3. **"Running Openclaw + Ollama locally with old hardware"** (18 upvotes, 15 comments)
4. **"Ollama and Openclaw on separate dedicated, isolated, firewalled machines"** (6 upvotes, 11 comments)

---

## 6. GitHub Integration Deep Dive

### OpenClaw GitHub Features
From the official documentation and GitHub repository:

**Repository:** https://github.com/openclaw/openclaw
- **Stars:** 163k+ (very popular project)
- **Forks:** 14.7k
- **Issues:** 1,954 active
- **Pull Requests:** 529

### GitHub Integration Capabilities

**1. Slash Commands for GitHub Operations**
OpenClaw supports extensive slash commands that can interact with GitHub:
- `/skill <name>` - Run skills (can include GitHub-related skills)
- `/config` - Read/write config (can store GitHub tokens/settings)
- `/subagents` - Spawn sub-agents for GitHub automation
- `/acp` - ACP (Agent Communication Protocol) for multi-agent coordination

**2. GitHub as a Skill Source**
Skills can be loaded from GitHub repositories:
```yaml
# In SKILL.md, skills can reference GitHub resources
metadata:
  openclaw:
    requires:
      repos: ["github.com/user/repo"]
```

**3. Community Plugins**
- GitHub plugin available for repository operations
- Can monitor repos, create issues, pull requests
- Webhook integration for GitHub events

**4. GitHub Actions Integration**
OpenClaw can trigger GitHub Actions workflows:
- Via webhook calls
- Via API integration
- For CI/CD automation

### Potential GitHub Workflows

**Workflow 1: Automated Documentation**
```
1. Agent writes documentation
2. Commits to GitHub repo
3. Triggers GitHub Pages deploy
4. Updates live docs automatically
```

**Workflow 2: Skill Distribution**
```
1. Create skill in workspace
2. Push to GitHub repo
3. Others can install via: clawhub install github:user/repo
```

**Workflow 3: Backup & Sync**
```
1. Cron job triggers daily
2. Agent commits workspace to GitHub
3. Includes memory files, configs
4. Version-controlled backup
```

**Workflow 4: Issue Management**
```
1. Monitor GitHub issues via webhook
2. Agent triages and responds
3. Creates PRs for fixes
4. Automated project management
```

---

## 7. Ollama API Technical Details

### API Endpoints (from GitHub docs)

**Key Endpoints for Integration:**
- `POST /api/generate` - Generate completion
- `POST /api/chat` - Chat completion
- `POST /api/create` - Create model
- `GET /api/tags` - List local models
- `POST /api/pull` - Pull model from registry
- `POST /api/push` - Push model to registry
- `POST /api/embed` - Generate embeddings
- `GET /api/ps` - List running models

### Critical Parameters for OpenClaw Integration

**Structured Outputs (JSON Mode):**
```json
{
  "model": "llama3.1:8b",
  "prompt": "Respond using JSON",
  "format": {
    "type": "object",
    "properties": {
      "tool_calls": { "type": "array" }
    }
  }
}
```
**Key for OpenClaw:** This could solve the tool calling issue with llama3.2:3b

**Raw Mode:**
```json
{
  "model": "mistral",
  "prompt": "[INST] why is the sky blue? [/INST]",
  "raw": true
}
```
**Key for OpenClaw:** Bypasses templating - may help with agent compatibility

**Context Management:**
- `context` parameter for conversational memory
- `keep_alive` for model persistence (default 5m)
- `num_ctx` for context window size

### Performance Metrics Available

Every response includes:
- `total_duration` - Total time
- `load_duration` - Model load time
- `prompt_eval_count` - Input tokens
- `prompt_eval_duration` - Prompt processing time
- `eval_count` - Output tokens
- `eval_duration` - Generation time

**Token/s calculation:** `eval_count / eval_duration * 10^9`

---

## 8. Alternative Backends Comparison

### Ollama vs llama.cpp vs vLLM

| Feature | Ollama | llama.cpp | vLLM |
|---------|--------|-----------|------|
| **Ease of Use** |  Easy |  Moderate |  Complex |
| **Speed** |  Moderate |  Fast |  Fastest |
| **API Compatibility** |  OpenAI-like |  Custom |  OpenAI-compatible |
| **Tool Calling** |  Limited |  Limited |  Better |
| **Context Window** |  Configurable |  Configurable |  Excellent |
| **GPU Support** |  Yes |  Yes |  Optimized |
| **Windows Support** |  Native |  WSL/Cygwin |  WSL |
| **OpenClaw Integration** |  Direct |  Needs wrapper |  Needs setup |

### Recommendation for Our System

**Current Best Choice: Ollama**
- Native Windows support
- Easiest setup
- Good enough for development
- Direct OpenClaw integration

**Future Upgrade Path: vLLM**
- Best performance
- Excellent for production
- Requires Linux/WSL
- More complex setup

**Not Recommended: llama.cpp**
- Windows support is poor
- Would require WSL
- No significant advantage over Ollama for our use case

---

## 9. Updated Action Items

### Immediate (This Week)
- [x] Document research findings 
- [ ] Switch local automation jobs to `agent:main`
- [ ] Test with `ollama/qwen2.5:7b`
- [ ] Try JSON mode for tool calling
- [ ] Test larger context windows

### Short-term (Next 2 Weeks)
- [ ] Create GitHub backup workflow
- [ ] Set up skill publishing to GitHub
- [ ] Research Kilo Code integration
- [ ] Set up Brave API key for web_search
- [ ] Benchmark Ollama performance

### Long-term (Next Month)
- [ ] Create custom skills for common tasks
- [ ] Publish useful skills to ClawHub
- [ ] Consider vLLM migration (Linux/WSL)
- [ ] Implement GitHub Actions automation
- [ ] Set up automated backup system

---

## 10. Resources & References

### OpenClaw
- **ClawHub**: https://clawhub.com
- **Docs**: https://docs.openclaw.ai
- **GitHub**: https://github.com/openclaw/openclaw
- **Skills Spec**: https://agentskills.io

### Ollama
- **GitHub**: https://github.com/ollama/ollama
- **API Docs**: https://docs.ollama.com/api
- **Models**: https://ollama.com/library

### Community
- **Reddit r/ollama**: https://reddit.com/r/ollama
- **Reddit r/openclaw**: https://reddit.com/r/openclaw
- **Discord**: OpenClaw community server

### Alternative Backends
- **llama.cpp**: https://github.com/ggerganov/llama.cpp
- **vLLM**: https://github.com/vllm-project/vllm
- **LM Studio**: https://lmstudio.ai
- **Kilo Code**: https://kilocode.ai

---

## 11. System Architecture Decision

### Recommended Setup for DESKTOP-M8AO8LN

```

           Windows 11 (DESKTOP-M8AO8LN)      
      
        OpenClaw Gateway 2026.2.25         
            
      main agent (Kimi K2.5)             
      - Primary interface                
      - GitHub integration               
            
            
      local-automation agent             
      - Sandboxed tasks only             
      - No Ollama access                 
            
      
                                             
      
    Ollama (localhost:11434)               
    - qwen2.5:7b (primary)                 
    - llama3.2:8b (backup)                 
    - JSON mode enabled                    
    - Large context window                 
      
                                             
      
    VNC Server (localhost:5900)            
    - Screenshot capability              
    - Mouse control                      
    - Keyboard control                   
      
                                             
      
    Git Integration                        
    - Local workspace repo                 
    - GitHub remote (future)               
    - Automated commits                    
      

```

### Key Design Decisions

1. **Use `agent:main` for Ollama jobs** - Bypasses sandbox, enables local LLM
2. **Keep `local-automation` for sandboxed tasks** - Security isolation
3. **qwen2.5:7b as primary model** - Best tool support
4. **Large context windows** - Community proven, not OP's advice
5. **JSON mode for structured outputs** - May fix tool calling
6. **GitHub integration for backup** - Version control everything
7. **VNC for screen control** - Already working

---

**Research Status:**  COMPLETE - All phases finished  
**Next Step:** Implementation based on findings

---

*Document created: 2026-02-26*  
*Last updated: 2026-02-26 (GitHub + Ollama API research added)*
